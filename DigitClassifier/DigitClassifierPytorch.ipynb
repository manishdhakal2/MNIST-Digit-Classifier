{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fb4a1bd-4472-467d-8535-4ef7b33d01f8",
   "metadata": {},
   "source": [
    "<h1>MNIST DIGIT CLASSIFICATION USING A PYTORCH NEURAL NETWORK</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea52ca0b-fd36-4aa9-9557-a3efe2cda76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary modules\n",
    "import torch\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\" #Set the device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "034fcfb9-8e31-426e-81f5-c52ee21369c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load The Datasets\n",
    "trainSet=pd.read_csv(r\"D:\\Datasets\\MNIST\\mnist_train.csv\")\n",
    "testSet=pd.read_csv(r\"D:\\Datasets\\MNIST\\mnist_test.csv\")\n",
    "\n",
    "#Since we have separate datasets for training and testing , we donot need to perform train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7af0142-3388-4a71-bb27-3c92ea48fd78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c26496e4-8804-4119-b9d4-cb660487f0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 784]) torch.Size([60000])\n"
     ]
    }
   ],
   "source": [
    "#Converting Df Into tensors\n",
    "train_x,train_y=torch.from_numpy(np.array(trainSet.iloc[:,1:]))/255.0,torch.from_numpy(np.array(trainSet.iloc[:,0]))\n",
    "train_x.to(device)\n",
    "train_y.to(device)\n",
    "test_x,test_y=torch.from_numpy(np.array(testSet.iloc[:,1:]))/255.0,torch.from_numpy(np.array(testSet.iloc[:,0]))\n",
    "print(train_x.shape,train_y.shape)\n",
    "\n",
    "\n",
    "#One hot encoding the train_y\n",
    "def oneHEncoding(data,size):\n",
    "    return torch.eye(size)[data];\n",
    "train_y=oneHEncoding(train_y,10)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8c761876-7464-4f33-95c0-478ece1567ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the model\n",
    "class DigitClassifier(nn.Module):\n",
    "    def __init__(self,lr,epochs,no_of_neurons):\n",
    "        super().__init__()\n",
    "        self.lr=lr\n",
    "        self.epochs=epochs\n",
    "        self.l1=nn.Linear(784,no_of_neurons)\n",
    "        self.l2=nn.Linear(no_of_neurons,no_of_neurons)\n",
    "        self.l3=nn.Linear(no_of_neurons,10)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.optimizer=torch.optim.SGD(params=self.parameters(),lr=self.lr,momentum=0.9)\n",
    "        torch.nn.init.kaiming_uniform_(self.l1.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.l2.weight, nonlinearity='relu')\n",
    "        torch.nn.init.kaiming_uniform_(self.l3.weight, nonlinearity='relu')\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.relu(self.l1(x))\n",
    "        x=self.relu(self.l2(x))\n",
    "        return self.l3(x)\n",
    "\n",
    "    def fit(self,x,y):\n",
    "        self.train()\n",
    "        loss_fn=nn.CrossEntropyLoss()\n",
    "        for i in range(self.epochs):\n",
    "            self.train()\n",
    "            print(f\"Iteration {i}\")\n",
    "            \n",
    "            y_pred=self.forward(x)\n",
    "            loss=loss_fn(y_pred,y)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.eval()\n",
    "            with torch.inference_mode():\n",
    "                if(i%10==0):\n",
    "                    print(f\"Loss={loss}\")\n",
    "            self.train()\n",
    "\n",
    "                    \n",
    "                \n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fe873ab4-5afb-4a47-ab1a-5116e9bdaa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Loss=2.416447877883911\n",
      "Iteration 1\n",
      "Iteration 2\n",
      "Iteration 3\n",
      "Iteration 4\n",
      "Iteration 5\n",
      "Iteration 6\n",
      "Iteration 7\n",
      "Iteration 8\n",
      "Iteration 9\n",
      "Iteration 10\n",
      "Loss=1.579172968864441\n",
      "Iteration 11\n",
      "Iteration 12\n",
      "Iteration 13\n",
      "Iteration 14\n",
      "Iteration 15\n",
      "Iteration 16\n",
      "Iteration 17\n",
      "Iteration 18\n",
      "Iteration 19\n",
      "Iteration 20\n",
      "Loss=0.8556860685348511\n",
      "Iteration 21\n",
      "Iteration 22\n",
      "Iteration 23\n",
      "Iteration 24\n",
      "Iteration 25\n",
      "Iteration 26\n",
      "Iteration 27\n",
      "Iteration 28\n",
      "Iteration 29\n",
      "Iteration 30\n",
      "Loss=0.638761043548584\n",
      "Iteration 31\n",
      "Iteration 32\n",
      "Iteration 33\n",
      "Iteration 34\n",
      "Iteration 35\n",
      "Iteration 36\n",
      "Iteration 37\n",
      "Iteration 38\n",
      "Iteration 39\n",
      "Iteration 40\n",
      "Loss=0.4863605201244354\n",
      "Iteration 41\n",
      "Iteration 42\n",
      "Iteration 43\n",
      "Iteration 44\n",
      "Iteration 45\n",
      "Iteration 46\n",
      "Iteration 47\n",
      "Iteration 48\n",
      "Iteration 49\n",
      "Iteration 50\n",
      "Loss=0.4108235538005829\n",
      "Iteration 51\n",
      "Iteration 52\n",
      "Iteration 53\n",
      "Iteration 54\n",
      "Iteration 55\n",
      "Iteration 56\n",
      "Iteration 57\n",
      "Iteration 58\n",
      "Iteration 59\n",
      "Iteration 60\n",
      "Loss=0.36947065591812134\n",
      "Iteration 61\n",
      "Iteration 62\n",
      "Iteration 63\n",
      "Iteration 64\n",
      "Iteration 65\n",
      "Iteration 66\n",
      "Iteration 67\n",
      "Iteration 68\n",
      "Iteration 69\n",
      "Iteration 70\n",
      "Loss=0.3391488790512085\n",
      "Iteration 71\n",
      "Iteration 72\n",
      "Iteration 73\n",
      "Iteration 74\n",
      "Iteration 75\n",
      "Iteration 76\n",
      "Iteration 77\n",
      "Iteration 78\n",
      "Iteration 79\n",
      "Iteration 80\n",
      "Loss=0.3169284164905548\n",
      "Iteration 81\n",
      "Iteration 82\n",
      "Iteration 83\n",
      "Iteration 84\n",
      "Iteration 85\n",
      "Iteration 86\n",
      "Iteration 87\n",
      "Iteration 88\n",
      "Iteration 89\n",
      "Iteration 90\n",
      "Loss=0.2991032004356384\n",
      "Iteration 91\n",
      "Iteration 92\n",
      "Iteration 93\n",
      "Iteration 94\n",
      "Iteration 95\n",
      "Iteration 96\n",
      "Iteration 97\n",
      "Iteration 98\n",
      "Iteration 99\n",
      "Iteration 100\n",
      "Loss=0.2839783728122711\n",
      "Iteration 101\n",
      "Iteration 102\n",
      "Iteration 103\n",
      "Iteration 104\n",
      "Iteration 105\n",
      "Iteration 106\n",
      "Iteration 107\n",
      "Iteration 108\n",
      "Iteration 109\n",
      "Iteration 110\n",
      "Loss=0.2711721956729889\n",
      "Iteration 111\n",
      "Iteration 112\n",
      "Iteration 113\n",
      "Iteration 114\n",
      "Iteration 115\n",
      "Iteration 116\n",
      "Iteration 117\n",
      "Iteration 118\n",
      "Iteration 119\n",
      "Iteration 120\n",
      "Loss=0.2600491940975189\n",
      "Iteration 121\n",
      "Iteration 122\n",
      "Iteration 123\n",
      "Iteration 124\n",
      "Iteration 125\n",
      "Iteration 126\n",
      "Iteration 127\n",
      "Iteration 128\n",
      "Iteration 129\n",
      "Iteration 130\n",
      "Loss=0.2503592371940613\n",
      "Iteration 131\n",
      "Iteration 132\n",
      "Iteration 133\n",
      "Iteration 134\n",
      "Iteration 135\n",
      "Iteration 136\n",
      "Iteration 137\n",
      "Iteration 138\n",
      "Iteration 139\n",
      "Iteration 140\n",
      "Loss=0.24194708466529846\n",
      "Iteration 141\n",
      "Iteration 142\n",
      "Iteration 143\n",
      "Iteration 144\n",
      "Iteration 145\n",
      "Iteration 146\n",
      "Iteration 147\n",
      "Iteration 148\n",
      "Iteration 149\n",
      "Iteration 150\n",
      "Loss=0.2345462590456009\n",
      "Iteration 151\n",
      "Iteration 152\n",
      "Iteration 153\n",
      "Iteration 154\n",
      "Iteration 155\n",
      "Iteration 156\n",
      "Iteration 157\n",
      "Iteration 158\n",
      "Iteration 159\n",
      "Iteration 160\n",
      "Loss=0.22800156474113464\n",
      "Iteration 161\n",
      "Iteration 162\n",
      "Iteration 163\n",
      "Iteration 164\n",
      "Iteration 165\n",
      "Iteration 166\n",
      "Iteration 167\n",
      "Iteration 168\n",
      "Iteration 169\n",
      "Iteration 170\n",
      "Loss=0.22222810983657837\n",
      "Iteration 171\n",
      "Iteration 172\n",
      "Iteration 173\n",
      "Iteration 174\n",
      "Iteration 175\n",
      "Iteration 176\n",
      "Iteration 177\n",
      "Iteration 178\n",
      "Iteration 179\n",
      "Iteration 180\n",
      "Loss=0.2170824408531189\n",
      "Iteration 181\n",
      "Iteration 182\n",
      "Iteration 183\n",
      "Iteration 184\n",
      "Iteration 185\n",
      "Iteration 186\n",
      "Iteration 187\n",
      "Iteration 188\n",
      "Iteration 189\n",
      "Iteration 190\n",
      "Loss=0.21248506009578705\n",
      "Iteration 191\n",
      "Iteration 192\n",
      "Iteration 193\n",
      "Iteration 194\n",
      "Iteration 195\n",
      "Iteration 196\n",
      "Iteration 197\n",
      "Iteration 198\n",
      "Iteration 199\n",
      "Iteration 200\n",
      "Loss=0.20831869542598724\n",
      "Iteration 201\n",
      "Iteration 202\n",
      "Iteration 203\n",
      "Iteration 204\n",
      "Iteration 205\n",
      "Iteration 206\n",
      "Iteration 207\n",
      "Iteration 208\n",
      "Iteration 209\n",
      "Iteration 210\n",
      "Loss=0.20453467965126038\n",
      "Iteration 211\n",
      "Iteration 212\n",
      "Iteration 213\n",
      "Iteration 214\n",
      "Iteration 215\n",
      "Iteration 216\n",
      "Iteration 217\n",
      "Iteration 218\n",
      "Iteration 219\n",
      "Iteration 220\n",
      "Loss=0.20106090605258942\n",
      "Iteration 221\n",
      "Iteration 222\n",
      "Iteration 223\n",
      "Iteration 224\n",
      "Iteration 225\n",
      "Iteration 226\n",
      "Iteration 227\n",
      "Iteration 228\n",
      "Iteration 229\n",
      "Iteration 230\n",
      "Loss=0.1978590190410614\n",
      "Iteration 231\n",
      "Iteration 232\n",
      "Iteration 233\n",
      "Iteration 234\n",
      "Iteration 235\n",
      "Iteration 236\n",
      "Iteration 237\n",
      "Iteration 238\n",
      "Iteration 239\n",
      "Iteration 240\n",
      "Loss=0.19491100311279297\n",
      "Iteration 241\n",
      "Iteration 242\n",
      "Iteration 243\n",
      "Iteration 244\n",
      "Iteration 245\n",
      "Iteration 246\n",
      "Iteration 247\n",
      "Iteration 248\n",
      "Iteration 249\n",
      "Iteration 250\n",
      "Loss=0.19217568635940552\n",
      "Iteration 251\n",
      "Iteration 252\n",
      "Iteration 253\n",
      "Iteration 254\n",
      "Iteration 255\n",
      "Iteration 256\n",
      "Iteration 257\n",
      "Iteration 258\n",
      "Iteration 259\n",
      "Iteration 260\n",
      "Loss=0.18962186574935913\n",
      "Iteration 261\n",
      "Iteration 262\n",
      "Iteration 263\n",
      "Iteration 264\n",
      "Iteration 265\n",
      "Iteration 266\n",
      "Iteration 267\n",
      "Iteration 268\n",
      "Iteration 269\n",
      "Iteration 270\n",
      "Loss=0.1872330904006958\n",
      "Iteration 271\n",
      "Iteration 272\n",
      "Iteration 273\n",
      "Iteration 274\n",
      "Iteration 275\n",
      "Iteration 276\n",
      "Iteration 277\n",
      "Iteration 278\n",
      "Iteration 279\n",
      "Iteration 280\n",
      "Loss=0.18497921526432037\n",
      "Iteration 281\n",
      "Iteration 282\n",
      "Iteration 283\n",
      "Iteration 284\n",
      "Iteration 285\n",
      "Iteration 286\n",
      "Iteration 287\n",
      "Iteration 288\n",
      "Iteration 289\n",
      "Iteration 290\n",
      "Loss=0.18283310532569885\n",
      "Iteration 291\n",
      "Iteration 292\n",
      "Iteration 293\n",
      "Iteration 294\n",
      "Iteration 295\n",
      "Iteration 296\n",
      "Iteration 297\n",
      "Iteration 298\n",
      "Iteration 299\n",
      "Iteration 300\n",
      "Loss=0.18079528212547302\n",
      "Iteration 301\n",
      "Iteration 302\n",
      "Iteration 303\n",
      "Iteration 304\n",
      "Iteration 305\n",
      "Iteration 306\n",
      "Iteration 307\n",
      "Iteration 308\n",
      "Iteration 309\n",
      "Iteration 310\n",
      "Loss=0.1788475215435028\n",
      "Iteration 311\n",
      "Iteration 312\n",
      "Iteration 313\n",
      "Iteration 314\n",
      "Iteration 315\n",
      "Iteration 316\n",
      "Iteration 317\n",
      "Iteration 318\n",
      "Iteration 319\n",
      "Iteration 320\n",
      "Loss=0.17697709798812866\n",
      "Iteration 321\n",
      "Iteration 322\n",
      "Iteration 323\n",
      "Iteration 324\n",
      "Iteration 325\n",
      "Iteration 326\n",
      "Iteration 327\n",
      "Iteration 328\n",
      "Iteration 329\n",
      "Iteration 330\n",
      "Loss=0.1752031147480011\n",
      "Iteration 331\n",
      "Iteration 332\n",
      "Iteration 333\n",
      "Iteration 334\n",
      "Iteration 335\n",
      "Iteration 336\n",
      "Iteration 337\n",
      "Iteration 338\n",
      "Iteration 339\n",
      "Iteration 340\n",
      "Loss=0.1735174059867859\n",
      "Iteration 341\n",
      "Iteration 342\n",
      "Iteration 343\n",
      "Iteration 344\n",
      "Iteration 345\n",
      "Iteration 346\n",
      "Iteration 347\n",
      "Iteration 348\n",
      "Iteration 349\n",
      "Iteration 350\n",
      "Loss=0.17189687490463257\n",
      "Iteration 351\n",
      "Iteration 352\n",
      "Iteration 353\n",
      "Iteration 354\n",
      "Iteration 355\n",
      "Iteration 356\n",
      "Iteration 357\n",
      "Iteration 358\n",
      "Iteration 359\n",
      "Iteration 360\n",
      "Loss=0.17034636437892914\n",
      "Iteration 361\n",
      "Iteration 362\n",
      "Iteration 363\n",
      "Iteration 364\n",
      "Iteration 365\n",
      "Iteration 366\n",
      "Iteration 367\n",
      "Iteration 368\n",
      "Iteration 369\n",
      "Iteration 370\n",
      "Loss=0.16886383295059204\n",
      "Iteration 371\n",
      "Iteration 372\n",
      "Iteration 373\n",
      "Iteration 374\n",
      "Iteration 375\n",
      "Iteration 376\n",
      "Iteration 377\n",
      "Iteration 378\n",
      "Iteration 379\n",
      "Iteration 380\n",
      "Loss=0.16744230687618256\n",
      "Iteration 381\n",
      "Iteration 382\n",
      "Iteration 383\n",
      "Iteration 384\n",
      "Iteration 385\n",
      "Iteration 386\n",
      "Iteration 387\n",
      "Iteration 388\n",
      "Iteration 389\n",
      "Iteration 390\n",
      "Loss=0.16607354581356049\n",
      "Iteration 391\n",
      "Iteration 392\n",
      "Iteration 393\n",
      "Iteration 394\n",
      "Iteration 395\n",
      "Iteration 396\n",
      "Iteration 397\n",
      "Iteration 398\n",
      "Iteration 399\n",
      "Iteration 400\n",
      "Loss=0.16475838422775269\n",
      "Iteration 401\n",
      "Iteration 402\n",
      "Iteration 403\n",
      "Iteration 404\n",
      "Iteration 405\n",
      "Iteration 406\n",
      "Iteration 407\n",
      "Iteration 408\n",
      "Iteration 409\n",
      "Iteration 410\n",
      "Loss=0.1634737104177475\n",
      "Iteration 411\n",
      "Iteration 412\n",
      "Iteration 413\n",
      "Iteration 414\n",
      "Iteration 415\n",
      "Iteration 416\n",
      "Iteration 417\n",
      "Iteration 418\n",
      "Iteration 419\n",
      "Iteration 420\n",
      "Loss=0.16223521530628204\n",
      "Iteration 421\n",
      "Iteration 422\n",
      "Iteration 423\n",
      "Iteration 424\n",
      "Iteration 425\n",
      "Iteration 426\n",
      "Iteration 427\n",
      "Iteration 428\n",
      "Iteration 429\n",
      "Iteration 430\n",
      "Loss=0.16103997826576233\n",
      "Iteration 431\n",
      "Iteration 432\n",
      "Iteration 433\n",
      "Iteration 434\n",
      "Iteration 435\n",
      "Iteration 436\n",
      "Iteration 437\n",
      "Iteration 438\n",
      "Iteration 439\n",
      "Iteration 440\n",
      "Loss=0.15987230837345123\n",
      "Iteration 441\n",
      "Iteration 442\n",
      "Iteration 443\n",
      "Iteration 444\n",
      "Iteration 445\n",
      "Iteration 446\n",
      "Iteration 447\n",
      "Iteration 448\n",
      "Iteration 449\n",
      "Iteration 450\n",
      "Loss=0.15874558687210083\n",
      "Iteration 451\n",
      "Iteration 452\n",
      "Iteration 453\n",
      "Iteration 454\n",
      "Iteration 455\n",
      "Iteration 456\n",
      "Iteration 457\n",
      "Iteration 458\n",
      "Iteration 459\n",
      "Iteration 460\n",
      "Loss=0.15764889121055603\n",
      "Iteration 461\n",
      "Iteration 462\n",
      "Iteration 463\n",
      "Iteration 464\n",
      "Iteration 465\n",
      "Iteration 466\n",
      "Iteration 467\n",
      "Iteration 468\n",
      "Iteration 469\n",
      "Iteration 470\n",
      "Loss=0.15659010410308838\n",
      "Iteration 471\n",
      "Iteration 472\n",
      "Iteration 473\n",
      "Iteration 474\n",
      "Iteration 475\n",
      "Iteration 476\n",
      "Iteration 477\n",
      "Iteration 478\n",
      "Iteration 479\n",
      "Iteration 480\n",
      "Loss=0.15556614100933075\n",
      "Iteration 481\n",
      "Iteration 482\n",
      "Iteration 483\n",
      "Iteration 484\n",
      "Iteration 485\n",
      "Iteration 486\n",
      "Iteration 487\n",
      "Iteration 488\n",
      "Iteration 489\n",
      "Iteration 490\n",
      "Loss=0.15457502007484436\n",
      "Iteration 491\n",
      "Iteration 492\n",
      "Iteration 493\n",
      "Iteration 494\n",
      "Iteration 495\n",
      "Iteration 496\n",
      "Iteration 497\n",
      "Iteration 498\n",
      "Iteration 499\n",
      "Iteration 500\n",
      "Loss=0.153609961271286\n",
      "Iteration 501\n",
      "Iteration 502\n",
      "Iteration 503\n",
      "Iteration 504\n",
      "Iteration 505\n",
      "Iteration 506\n",
      "Iteration 507\n",
      "Iteration 508\n",
      "Iteration 509\n",
      "Iteration 510\n",
      "Loss=0.1526675522327423\n",
      "Iteration 511\n",
      "Iteration 512\n",
      "Iteration 513\n",
      "Iteration 514\n",
      "Iteration 515\n",
      "Iteration 516\n",
      "Iteration 517\n",
      "Iteration 518\n",
      "Iteration 519\n",
      "Iteration 520\n",
      "Loss=0.15175707638263702\n",
      "Iteration 521\n",
      "Iteration 522\n",
      "Iteration 523\n",
      "Iteration 524\n",
      "Iteration 525\n",
      "Iteration 526\n",
      "Iteration 527\n",
      "Iteration 528\n",
      "Iteration 529\n",
      "Iteration 530\n",
      "Loss=0.15087011456489563\n",
      "Iteration 531\n",
      "Iteration 532\n",
      "Iteration 533\n",
      "Iteration 534\n",
      "Iteration 535\n",
      "Iteration 536\n",
      "Iteration 537\n",
      "Iteration 538\n",
      "Iteration 539\n",
      "Iteration 540\n",
      "Loss=0.15001341700553894\n",
      "Iteration 541\n",
      "Iteration 542\n",
      "Iteration 543\n",
      "Iteration 544\n",
      "Iteration 545\n",
      "Iteration 546\n",
      "Iteration 547\n",
      "Iteration 548\n",
      "Iteration 549\n",
      "Iteration 550\n",
      "Loss=0.14918172359466553\n",
      "Iteration 551\n",
      "Iteration 552\n",
      "Iteration 553\n",
      "Iteration 554\n",
      "Iteration 555\n",
      "Iteration 556\n",
      "Iteration 557\n",
      "Iteration 558\n",
      "Iteration 559\n",
      "Iteration 560\n",
      "Loss=0.14837507903575897\n",
      "Iteration 561\n",
      "Iteration 562\n",
      "Iteration 563\n",
      "Iteration 564\n",
      "Iteration 565\n",
      "Iteration 566\n",
      "Iteration 567\n",
      "Iteration 568\n",
      "Iteration 569\n",
      "Iteration 570\n",
      "Loss=0.14758531749248505\n",
      "Iteration 571\n",
      "Iteration 572\n",
      "Iteration 573\n",
      "Iteration 574\n",
      "Iteration 575\n",
      "Iteration 576\n",
      "Iteration 577\n",
      "Iteration 578\n",
      "Iteration 579\n",
      "Iteration 580\n",
      "Loss=0.14680954813957214\n",
      "Iteration 581\n",
      "Iteration 582\n",
      "Iteration 583\n",
      "Iteration 584\n",
      "Iteration 585\n",
      "Iteration 586\n",
      "Iteration 587\n",
      "Iteration 588\n",
      "Iteration 589\n",
      "Iteration 590\n",
      "Loss=0.14604584872722626\n",
      "Iteration 591\n",
      "Iteration 592\n",
      "Iteration 593\n",
      "Iteration 594\n",
      "Iteration 595\n",
      "Iteration 596\n",
      "Iteration 597\n",
      "Iteration 598\n",
      "Iteration 599\n",
      "Iteration 600\n",
      "Loss=0.14529511332511902\n",
      "Iteration 601\n",
      "Iteration 602\n",
      "Iteration 603\n",
      "Iteration 604\n",
      "Iteration 605\n",
      "Iteration 606\n",
      "Iteration 607\n",
      "Iteration 608\n",
      "Iteration 609\n",
      "Iteration 610\n",
      "Loss=0.14455606043338776\n",
      "Iteration 611\n",
      "Iteration 612\n",
      "Iteration 613\n",
      "Iteration 614\n",
      "Iteration 615\n",
      "Iteration 616\n",
      "Iteration 617\n",
      "Iteration 618\n",
      "Iteration 619\n",
      "Iteration 620\n",
      "Loss=0.14382824301719666\n",
      "Iteration 621\n",
      "Iteration 622\n",
      "Iteration 623\n",
      "Iteration 624\n",
      "Iteration 625\n",
      "Iteration 626\n",
      "Iteration 627\n",
      "Iteration 628\n",
      "Iteration 629\n",
      "Iteration 630\n",
      "Loss=0.14311054348945618\n",
      "Iteration 631\n",
      "Iteration 632\n",
      "Iteration 633\n",
      "Iteration 634\n",
      "Iteration 635\n",
      "Iteration 636\n",
      "Iteration 637\n",
      "Iteration 638\n",
      "Iteration 639\n",
      "Iteration 640\n",
      "Loss=0.14240655303001404\n",
      "Iteration 641\n",
      "Iteration 642\n",
      "Iteration 643\n",
      "Iteration 644\n",
      "Iteration 645\n",
      "Iteration 646\n",
      "Iteration 647\n",
      "Iteration 648\n",
      "Iteration 649\n",
      "Iteration 650\n",
      "Loss=0.14172038435935974\n",
      "Iteration 651\n",
      "Iteration 652\n",
      "Iteration 653\n",
      "Iteration 654\n",
      "Iteration 655\n",
      "Iteration 656\n",
      "Iteration 657\n",
      "Iteration 658\n",
      "Iteration 659\n",
      "Iteration 660\n",
      "Loss=0.1410500556230545\n",
      "Iteration 661\n",
      "Iteration 662\n",
      "Iteration 663\n",
      "Iteration 664\n",
      "Iteration 665\n",
      "Iteration 666\n",
      "Iteration 667\n",
      "Iteration 668\n",
      "Iteration 669\n",
      "Iteration 670\n",
      "Loss=0.14039528369903564\n",
      "Iteration 671\n",
      "Iteration 672\n",
      "Iteration 673\n",
      "Iteration 674\n",
      "Iteration 675\n",
      "Iteration 676\n",
      "Iteration 677\n",
      "Iteration 678\n",
      "Iteration 679\n",
      "Iteration 680\n",
      "Loss=0.13975434005260468\n",
      "Iteration 681\n",
      "Iteration 682\n",
      "Iteration 683\n",
      "Iteration 684\n",
      "Iteration 685\n",
      "Iteration 686\n",
      "Iteration 687\n",
      "Iteration 688\n",
      "Iteration 689\n",
      "Iteration 690\n",
      "Loss=0.13912217319011688\n",
      "Iteration 691\n",
      "Iteration 692\n",
      "Iteration 693\n",
      "Iteration 694\n",
      "Iteration 695\n",
      "Iteration 696\n",
      "Iteration 697\n",
      "Iteration 698\n",
      "Iteration 699\n",
      "Iteration 700\n",
      "Loss=0.13850145041942596\n",
      "Iteration 701\n",
      "Iteration 702\n",
      "Iteration 703\n",
      "Iteration 704\n",
      "Iteration 705\n",
      "Iteration 706\n",
      "Iteration 707\n",
      "Iteration 708\n",
      "Iteration 709\n",
      "Iteration 710\n",
      "Loss=0.13789339363574982\n",
      "Iteration 711\n",
      "Iteration 712\n",
      "Iteration 713\n",
      "Iteration 714\n",
      "Iteration 715\n",
      "Iteration 716\n",
      "Iteration 717\n",
      "Iteration 718\n",
      "Iteration 719\n",
      "Iteration 720\n",
      "Loss=0.13729920983314514\n",
      "Iteration 721\n",
      "Iteration 722\n",
      "Iteration 723\n",
      "Iteration 724\n",
      "Iteration 725\n",
      "Iteration 726\n",
      "Iteration 727\n",
      "Iteration 728\n",
      "Iteration 729\n",
      "Iteration 730\n",
      "Loss=0.13671144843101501\n",
      "Iteration 731\n",
      "Iteration 732\n",
      "Iteration 733\n",
      "Iteration 734\n",
      "Iteration 735\n",
      "Iteration 736\n",
      "Iteration 737\n",
      "Iteration 738\n",
      "Iteration 739\n",
      "Iteration 740\n",
      "Loss=0.13612820208072662\n",
      "Iteration 741\n",
      "Iteration 742\n",
      "Iteration 743\n",
      "Iteration 744\n",
      "Iteration 745\n",
      "Iteration 746\n",
      "Iteration 747\n",
      "Iteration 748\n",
      "Iteration 749\n",
      "Iteration 750\n",
      "Loss=0.13555537164211273\n",
      "Iteration 751\n",
      "Iteration 752\n",
      "Iteration 753\n",
      "Iteration 754\n",
      "Iteration 755\n",
      "Iteration 756\n",
      "Iteration 757\n",
      "Iteration 758\n",
      "Iteration 759\n",
      "Iteration 760\n",
      "Loss=0.13499140739440918\n",
      "Iteration 761\n",
      "Iteration 762\n",
      "Iteration 763\n",
      "Iteration 764\n",
      "Iteration 765\n",
      "Iteration 766\n",
      "Iteration 767\n",
      "Iteration 768\n",
      "Iteration 769\n",
      "Iteration 770\n",
      "Loss=0.13442300260066986\n",
      "Iteration 771\n",
      "Iteration 772\n",
      "Iteration 773\n",
      "Iteration 774\n",
      "Iteration 775\n",
      "Iteration 776\n",
      "Iteration 777\n",
      "Iteration 778\n",
      "Iteration 779\n",
      "Iteration 780\n",
      "Loss=0.13386011123657227\n",
      "Iteration 781\n",
      "Iteration 782\n",
      "Iteration 783\n",
      "Iteration 784\n",
      "Iteration 785\n",
      "Iteration 786\n",
      "Iteration 787\n",
      "Iteration 788\n",
      "Iteration 789\n",
      "Iteration 790\n",
      "Loss=0.1333022266626358\n",
      "Iteration 791\n",
      "Iteration 792\n",
      "Iteration 793\n",
      "Iteration 794\n",
      "Iteration 795\n",
      "Iteration 796\n",
      "Iteration 797\n",
      "Iteration 798\n",
      "Iteration 799\n",
      "Iteration 800\n",
      "Loss=0.1327555924654007\n",
      "Iteration 801\n",
      "Iteration 802\n",
      "Iteration 803\n",
      "Iteration 804\n",
      "Iteration 805\n",
      "Iteration 806\n",
      "Iteration 807\n",
      "Iteration 808\n",
      "Iteration 809\n",
      "Iteration 810\n",
      "Loss=0.13221904635429382\n",
      "Iteration 811\n",
      "Iteration 812\n",
      "Iteration 813\n",
      "Iteration 814\n",
      "Iteration 815\n",
      "Iteration 816\n",
      "Iteration 817\n",
      "Iteration 818\n",
      "Iteration 819\n",
      "Iteration 820\n",
      "Loss=0.1316925436258316\n",
      "Iteration 821\n",
      "Iteration 822\n",
      "Iteration 823\n",
      "Iteration 824\n",
      "Iteration 825\n",
      "Iteration 826\n",
      "Iteration 827\n",
      "Iteration 828\n",
      "Iteration 829\n",
      "Iteration 830\n",
      "Loss=0.13117264211177826\n",
      "Iteration 831\n",
      "Iteration 832\n",
      "Iteration 833\n",
      "Iteration 834\n",
      "Iteration 835\n",
      "Iteration 836\n",
      "Iteration 837\n",
      "Iteration 838\n",
      "Iteration 839\n",
      "Iteration 840\n",
      "Loss=0.13066165149211884\n",
      "Iteration 841\n",
      "Iteration 842\n",
      "Iteration 843\n",
      "Iteration 844\n",
      "Iteration 845\n",
      "Iteration 846\n",
      "Iteration 847\n",
      "Iteration 848\n",
      "Iteration 849\n",
      "Iteration 850\n",
      "Loss=0.1301540583372116\n",
      "Iteration 851\n",
      "Iteration 852\n",
      "Iteration 853\n",
      "Iteration 854\n",
      "Iteration 855\n",
      "Iteration 856\n",
      "Iteration 857\n",
      "Iteration 858\n",
      "Iteration 859\n",
      "Iteration 860\n",
      "Loss=0.12965519726276398\n",
      "Iteration 861\n",
      "Iteration 862\n",
      "Iteration 863\n",
      "Iteration 864\n",
      "Iteration 865\n",
      "Iteration 866\n",
      "Iteration 867\n",
      "Iteration 868\n",
      "Iteration 869\n",
      "Iteration 870\n",
      "Loss=0.1291658580303192\n",
      "Iteration 871\n",
      "Iteration 872\n",
      "Iteration 873\n",
      "Iteration 874\n",
      "Iteration 875\n",
      "Iteration 876\n",
      "Iteration 877\n",
      "Iteration 878\n",
      "Iteration 879\n",
      "Iteration 880\n",
      "Loss=0.12868809700012207\n",
      "Iteration 881\n",
      "Iteration 882\n",
      "Iteration 883\n",
      "Iteration 884\n",
      "Iteration 885\n",
      "Iteration 886\n",
      "Iteration 887\n",
      "Iteration 888\n",
      "Iteration 889\n",
      "Iteration 890\n",
      "Loss=0.12821708619594574\n",
      "Iteration 891\n",
      "Iteration 892\n",
      "Iteration 893\n",
      "Iteration 894\n",
      "Iteration 895\n",
      "Iteration 896\n",
      "Iteration 897\n",
      "Iteration 898\n",
      "Iteration 899\n",
      "Iteration 900\n",
      "Loss=0.1277543604373932\n",
      "Iteration 901\n",
      "Iteration 902\n",
      "Iteration 903\n",
      "Iteration 904\n",
      "Iteration 905\n",
      "Iteration 906\n",
      "Iteration 907\n",
      "Iteration 908\n",
      "Iteration 909\n",
      "Iteration 910\n",
      "Loss=0.12730100750923157\n",
      "Iteration 911\n",
      "Iteration 912\n",
      "Iteration 913\n",
      "Iteration 914\n",
      "Iteration 915\n",
      "Iteration 916\n",
      "Iteration 917\n",
      "Iteration 918\n",
      "Iteration 919\n",
      "Iteration 920\n",
      "Loss=0.12685726583003998\n",
      "Iteration 921\n",
      "Iteration 922\n",
      "Iteration 923\n",
      "Iteration 924\n",
      "Iteration 925\n",
      "Iteration 926\n",
      "Iteration 927\n",
      "Iteration 928\n",
      "Iteration 929\n",
      "Iteration 930\n",
      "Loss=0.12641973793506622\n",
      "Iteration 931\n",
      "Iteration 932\n",
      "Iteration 933\n",
      "Iteration 934\n",
      "Iteration 935\n",
      "Iteration 936\n",
      "Iteration 937\n",
      "Iteration 938\n",
      "Iteration 939\n",
      "Iteration 940\n",
      "Loss=0.12598678469657898\n",
      "Iteration 941\n",
      "Iteration 942\n",
      "Iteration 943\n",
      "Iteration 944\n",
      "Iteration 945\n",
      "Iteration 946\n",
      "Iteration 947\n",
      "Iteration 948\n",
      "Iteration 949\n",
      "Iteration 950\n",
      "Loss=0.125558003783226\n",
      "Iteration 951\n",
      "Iteration 952\n",
      "Iteration 953\n",
      "Iteration 954\n",
      "Iteration 955\n",
      "Iteration 956\n",
      "Iteration 957\n",
      "Iteration 958\n",
      "Iteration 959\n",
      "Iteration 960\n",
      "Loss=0.12513351440429688\n",
      "Iteration 961\n",
      "Iteration 962\n",
      "Iteration 963\n",
      "Iteration 964\n",
      "Iteration 965\n",
      "Iteration 966\n",
      "Iteration 967\n",
      "Iteration 968\n",
      "Iteration 969\n",
      "Iteration 970\n",
      "Loss=0.12471562623977661\n",
      "Iteration 971\n",
      "Iteration 972\n",
      "Iteration 973\n",
      "Iteration 974\n",
      "Iteration 975\n",
      "Iteration 976\n",
      "Iteration 977\n",
      "Iteration 978\n",
      "Iteration 979\n",
      "Iteration 980\n",
      "Loss=0.12430739402770996\n",
      "Iteration 981\n",
      "Iteration 982\n",
      "Iteration 983\n",
      "Iteration 984\n",
      "Iteration 985\n",
      "Iteration 986\n",
      "Iteration 987\n",
      "Iteration 988\n",
      "Iteration 989\n",
      "Iteration 990\n",
      "Loss=0.12390578538179398\n",
      "Iteration 991\n",
      "Iteration 992\n",
      "Iteration 993\n",
      "Iteration 994\n",
      "Iteration 995\n",
      "Iteration 996\n",
      "Iteration 997\n",
      "Iteration 998\n",
      "Iteration 999\n"
     ]
    }
   ],
   "source": [
    "#Creating an instance of  the model\n",
    "torch.manual_seed(42)\n",
    "model=DigitClassifier(0.1,1000,16)\n",
    "\n",
    "model.fit(train_x,train_y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "73648c84-5103-477b-bc51-531b3921ff75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:95.08999633789062\n"
     ]
    }
   ],
   "source": [
    "#Testing the model  on unseen data\n",
    "\n",
    "\n",
    "model.eval()\n",
    "y_pred=model.forward(test_x)\n",
    "y_labels=torch.argmax(y_pred,dim=1)\n",
    "correct=torch.sum(y_labels==test_y)\n",
    "print(f\"Accuracy:{correct/100.0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8ed61d-5b71-4790-9a9c-bf2fb343ab7c",
   "metadata": {},
   "source": [
    "### As Compared To The Raw Model before , this model performs slightly better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5267f54-9f4a-470f-9f33-d1b91046fe9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
